---
title: Language learnability variability
author: "Molly Lewis"
date: "`r Sys.Date()`"
output: 
  html_document:
  toc: true
  number_sections: true
---
  
***
***

```{r, echo = F, warning = F, message = F}
rm(list = ls())
library(knitr)
library(ggplot2)
library(langcog)
library(plyr)
library(dplyr)
library(tidyr)
library(corrplot)
library(broom)
library(lme4)

opts_chunk$set(cache = F, warning = F, message = F)

cor.mtest <- function(mat, conf.level = 0.95) {
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat <- lowCI.mat <- uppCI.mat <- matrix(NA, n, n)
  diag(p.mat) <- 0
  diag(lowCI.mat) <- diag(uppCI.mat) <- 1
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(mat[, i], mat[, j], conf.level = conf.level)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
      lowCI.mat[i, j] <- lowCI.mat[j, i] <- tmp$conf.int[1]
      uppCI.mat[i, j] <- uppCI.mat[j, i] <- tmp$conf.int[2]
      }
    }
  return(list(p.mat, lowCI.mat, uppCI.mat))
}

removeOutlier <- function(dataCol, num_deviations) {
  boolean.outlier = abs(dataCol - mean(dataCol, na.rm = T)) > 
                    num_deviations*sd(dataCol, na.rm = T)
  df = bind_cols(as.data.frame(boolean.outlier), as.data.frame(dataCol))
  cleaned.dataCol = ifelse(df$boolean.outlier, NA, df$dataCol)
  cleaned.dataCol
}
```

## Read in datasets
 * Lupyan & Dale (2010): Demographic variables and syntactic complexity
    + log population size (pop_log)
    + area (area)
    + number of bordering countries (n.neigbors)
    + mean temperature (mean.temp)
    + variability in temperature (sd.temp)
    + total precipitation (sum.precip)
    + variability in precipition (sd.precip)
    + growing season (growing.season)
    + latitude (lat)
    + longitude (long)
    + [syntactic complexity]
 * Bentz, et al. (2015): L2 learners and lexical diversity
    + number of first language speakers (n.L1.speakers)
    + number of second language speakers (n.L2.pseakers)
    + ratio of L2 to L1 (ratio.L2.L1)
    + parameters of Zipf-Mandelbrotâ€™s law for lexical diversity (scaled.LDT.ZM)
    + Shannon entropy of lexical diversity (scaled.LDT.H)
    + Type-token ratio as measure of lexical diversity (scaled.LDT.TTR)
 * Atkinson (2011): Phonemic diversity and best-fitting distance from origin
    + vowel diversity (normalized.vowel.diversity)
    + consonant diversity (normalized.consonant.diversity)
    + tone diversity (normalized.tone.diversity)
    + phoneme diversity (normalized.phoneme.diversity)
    + estimated distance for language origin (distance.from.origin)
 * Lewis & Frank (under review): Complexity bias
    + complexity bias, correlation between length and complexity (complexity.bias)
    + bias partially out spoken frequency (p.complexity.bias)
    + bias for only monosyllabic words (mono.complexity.bias)
    + bias for only open class words (open.complexity.bias)
 * Futrell, Mahowald, & Gibson (2015): Dependency length
    + mean observed dependency length in characters (mean.dependency.length)
 * Pellegrino, Coupe, & Marsico (2015): Information density
     + information.density
     + syllable.rate
     + information.rate
 * Moran, McCloy, & Wright (2012): Phonemic inventory
    + number of phonemes (n.phonemes)
    + number of cononsants (n.consonants)
    + number of vowels (n.vowels)
    + number of sonorants (n.sonorants)
    + number of obsruents (n.obsruents)
    + number of monophthongs  (n.monophthongs)
    + number of quality-only monophthong constrasts (n.qual.monophthongs)
    + number of tones (n.tones)
 * Wichmann, et al. (2013): Mean word length
    + mean length in number of characters (mean.length)
 * Luniewska, et al. (2015): Age of acquisition (aoa)
    + mean adult-judged age of acquistion (mean.aoa)

Language codes (from WALS) - WALS, ISO, ascii-name. These are used to merge across datasets.
```{r}
codes = read.csv("../data/language_codes.csv") %>%
  select(language, WALS, ISO) %>%
  mutate(ISO = unlist(lapply(strsplit(as.character(ISO),
                                      ","),function(x) x[1])))
# in cases where there are two ISO codes, takes first
```

Lupyan & Dale (2010): Demographic variables and syntactic compelxity
```{r}
ld = read.table("../data/lupyan_2010.txt", fill = T, 
               header = T, sep = "\t", na.strings = "*") %>%
    left_join(codes, c("walsCode" = "WALS")) %>%
    rename(pop_log = logpop2, 
           mean.temp = aveTemp,
           n.neighbors = numNeighbors,
           sum.precip = sumPrecip,
           sd.temp = sdTemp,
           sd.precip = sdPrecip,
           lang.family = langFamily,
           lang.genus = langGenus,
           native.country = nativeCountry,
           native.country.area = nativeCountryArea,
           growing.season = growingSeason)

# get means across language
demo_ld = ld %>%
     group_by(ISO) %>%
     summarise_each(funs(mean(., na.rm = TRUE)), 
                    c(8:9, 16:121))  %>%
     select(1:3, 93:95, 100:101, 103:105, 108)

# add in data family
fams = ld %>%
      group_by(ISO) %>%
      filter(row_number() == 1) %>%
      select(ISO, lang.family, lang.genus, native.country,
             native.country.area, language)  

demo_ld = left_join(fams, demo_ld, by = "ISO") %>%
          ungroup()
```

Bentz, et al. (2015): L2 learners and lexical diversity
Note here that we are only looking at languages from the UDHR corpus. The other corpora have smaller languages only.
```{r}
bentz = read.csv("../data/bentz_2015.csv") %>%
    gather(temp, LDT, starts_with("LDT")) %>% 
    unite(temp1, measure, temp, sep = ".") %>% 
    spread(temp1, LDT) %>%
    filter(text == "UDHR") %>%
    select(iso_639_3, RatioL2, a.LDTscaled, 
           H.LDTscaled, TTR.LDTscaled, Stock,
           Region, L1_speakers, L2_speakers) %>%
    rename(ISO = iso_639_3,
           n.L1.speakers = L1_speakers,
           n.L2.speakers = L2_speakers,
           ratio.L2.L1 = RatioL2,
           stock = Stock,
           region = Region,
           scaled.LDT.TTR = TTR.LDTscaled,
           scaled.LDT.ZM = a.LDTscaled,
           scaled.LDT.H = H.LDTscaled) %>%
    left_join(codes, by="ISO") %>%
    mutate(ISO = as.factor(ISO)) %>%
    select(-language, -WALS) 
  
```

Atkinson (2011)
```{r}
atkinson = read.csv("../data/atkinson_2011.csv") %>%
  select(normalized.vowel.diversity,
         normalized.consonant.diversity,	
         normalized.tone.diversity,
         normalized.phoneme.diversity, ISO, 
         distance.from.origin) %>%
  mutate(ISO = unlist(lapply(strsplit(as.character(ISO),
                                      " "),function(x) x[1]))) %>%
  left_join(codes, by="ISO") %>%
  select(-language, -WALS)
```

Lewis & Frank (under review): Complexity Bias
```{r}
# complexity bias
cb = read.csv("../data/lewis_2015.csv") %>%
      rename(complexity.bias = corr,
              p.complexity.bias = p.corr,
              mono.complexity.bias = mono.cor,
              open.complexity.bias = open.cor) %>%
      left_join(codes, by="language") %>%
    select(-X.1, -X, -lower.ci, -upper.ci, -checked,
           -mean.length) %>%
    distinct(ISO) %>%
   filter(language != "english") %>% # english is an outlier in this dataset because norms colelcted in english
  select(-language, -WALS)
```

Futrell, Mahowald, & Gibson (2015): Dependency length
```{r}
dl = read.csv("../data/futrell_2015.csv") %>%
  left_join(codes, by="language") %>%
  select(-language, -WALS, -fixed.random.baseline.slope, 
         -observed.slope, -m_rand) %>%
  rename(mean.dependency.length = m_obs)

```

Pellegrino, Coupe, & Marsico (2015): Information density
```{r}
uid = read.csv("../data/pellegrino_2015.csv")  %>%
  left_join(codes, by="language") %>%
  select(-language, -WALS)

```

Moran, McCloy, & Wright (2012): Phonemic inventory
```{r}
phoneme = read.csv("../data/moran_2012.csv") %>%
          select(ISO, pho, con, vow, son, 
                 obs, mon, qua, ton) %>%
          rename(n.phonemes = pho,
                 n.consonants = con,
                 n.vowels = vow,
                 n.sonorants = son,
                 n.obstruents = obs,
                 n.monophthongs = mon,
                 n.qual.monophthongs = qua,
                 n.tones = ton)
```

Wichmann, et al. (2013): Mean word length
```{r}
# note is this IPA? - fix this!
ml = read.csv("../data/wichmann_2013.csv") %>% 
  select(1,1:109) %>%
  gather(word,translation,I:name) %>%
  mutate(nchar = unlist(
    lapply(
      lapply(
        strsplit(
          gsub("[[:space:]]", "", translation) ,
          ","), 
           nchar), mean))) %>%
  filter(translation != "")

# subset to only those words in the swadesh list (n = 40)
swadesh.words  = ml[ml$ISO == "gwj", "word"] 
ml = ml %>%
      filter(is.element(word, swadesh.words))

ml.d = ml %>%
        group_by(ISO) %>%
        summarize(mean.length = mean(nchar, na.rm = T)) 
```

Luniewska, et al. (2015): Age of acquistion (Aoa)
```{r}
aoa.raw = read.csv("../data/luniewska_2015.csv", 
                       header = T, fileEncoding = "latin1")
  
aoa = aoa.raw %>%
  gather(language_aoa, aoa, grep("aoa", 
                                 names(aoa.raw))) %>%
  mutate(language_aoa = tolower(unlist(lapply(strsplit(
    as.character(language_aoa),"_"), function(x) x[1])))) %>%
  select(-3:-27) %>%
  rename(language = language_aoa) %>%
    left_join(codes, by="language") %>%
  group_by(ISO) %>%
  summarize(mean.aoa = mean(aoa, na.rm = T))
```

Merge data frames together by ISO
```{r }
d = full_join(demo_ld, cb, by = "ISO") %>%
    full_join(bentz, by = "ISO") %>%
    full_join(dl, by = "ISO") %>%
    full_join(uid, by = "ISO") %>%
    full_join(phoneme, by = "ISO") %>%
    full_join(ml.d, by = "ISO") %>%
    full_join(aoa, by = "ISO") %>%
    full_join(atkinson, by = "ISO") %>%
    distinct(ISO) %>%
    filter(!is.na(ISO), ISO !="") %>%
    mutate(ISO = as.factor(ISO))
```

Remove outliers and check for normality
```{r, fig.width = 15, fig.height = 10}
d.clean = d %>%
  mutate_each(funs(removeOutlier(.,3)), c(-ISO, -lang.family, 
              -native.country, -native.country.area, 
         -lang.genus, -language, -stock, -region))

d.clean %>%
  select(-ISO, -lang.family, -native.country, -native.country.area, 
         -lang.genus, -language, -stock, -region) %>%
  gather(var, value) %>%
  ggplot(aes(sample = value)) + 
  stat_qq(na.rm = T, size = .2) + 
  facet_wrap( ~ var, scales = "free") +
  theme_bw()
```

The following variables look right-skewed. 
```{r}
NN_vars = c("area", "perimeter", "n.neighbors", "sd.precip",
           "ratio.L2.L1", "n.L1.speakers", "n.L2.speakers",
           "n.phonemes", "n.consonants","n.vowels", "n.sonorants",
           "n.obstruents", "n.monophthongs", "n.qual.monophthongs", "n.tones")
```

Take the log of these variables, and check for normality again.
```{r, fig.width = 15, fig.height = 10}
d.clean = d.clean %>%
  mutate_each(funs(log = log(.), dummy =  sum(.)),
              one_of(NN_vars)) %>%
  select(-contains("dummy")) %>%
  select(-one_of(NNvars))

d.clean %>%
  select(-ISO, -lang.family, -native.country, -native.country.area, 
         -lang.genus, -language, -stock, -region) %>%
  gather(var, value) %>%
  ggplot(aes(sample = value)) + 
  stat_qq(na.rm = T, size = .2) + 
  facet_wrap( ~ var, scales = "free") +
  theme_bw()
```

Check for normality of variables using Shapiro-Wilk Normality Test
```{r}
is.na(d.clean) <- sapply(d.clean, is.infinite)

normality.test = d.clean %>%
  summarise_each(funs(unlist(tidy(shapiro.test(.))[2])),
                 c(-ISO, -lang.family, -native.country, -native.country.area, 
                     -lang.genus, -language, -stock, -region)) %>%
  gather(variable, shapiro.p.value) %>%
  mutate(normal = ifelse(shapiro.p.value > .05, TRUE, FALSE)) %>%
  summary()
```
By this test, only 14 variables normal. But, I think this is the best we can do (?). 

Look at histograms of all variables. 
```{r,  fig.height = 10, fig.width = 10, eval = F}
demo_vars = c("native.country.area", "lat", "lon", "perimeter_log", "n.neighbors_log", "mean.temp", "sum.precip", "sd.temp", "growing.season", "pop_log", "area_log", "perimeter_log", "n.neighbors_log", "sd.precip_log", "ratio.L2.L1_log", "n.L1.speakers_log", "n.L2.speakers_log", "distance.from.origin")

lang_vars = c(setdiff(names(d.clean), demo_vars))[-c(1:5, 13,14)]

d.clean %>%
  gather(variable, num, lat:length(d.clean))  %>%
  filter(variable != "stock", variable != "region") %>%
  mutate(var_type = ifelse(is.element(variable, demo_vars),
                           "demo", "lang"),
         num = as.numeric(as.character(num))) %>%
  arrange(var_type) %>% 
  mutate(variable = factor(variable, variable)) %>%
  ggplot(aes(x=num, fill = var_type)) + 
    geom_histogram(position = "identity") +
    facet_wrap( ~ variable, scales = "free") + 
    theme_bw()
```

Number of variables we have data for, for each language
```{r, fig.height = 6, eval = F}
# as is this plot is too hard to read
d.clean %>%
  gather(variable, num, c(7:21, 23:48))  %>%
  group_by(language) %>%
  summarize(counts = length(which(is.na(num) == F))) %>%
  filter(!is.na(counts), counts > 10, !is.na(language)) %>%
  ggplot(aes(y = counts, x = language)) + 
    geom_bar(stat = "identity") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    ylab("Number of variables (minimum 10)") +
    ggtitle("Number variables per language")
```

Number of languages we have data, for each variable
```{r, fig.height = 6}
d.clean %>%
  gather(variable, num, lat:length(d.clean))  %>%
  filter(variable != "stock", variable != "region") %>%

  group_by(variable) %>%
  summarize(counts = length(which(is.na(num) == F)))  %>%
  mutate(var_type = ifelse(is.element(variable, demo_vars),
                           "demo", "lang")) %>%
  mutate(counts_trunc = ifelse(counts > 150, 150, counts))  %>%
  arrange(var_type) %>% 
  mutate(variable = factor(variable, variable)) %>%
  ggplot(aes(y=counts_trunc, x = variable, fill = var_type)) + 
    geom_bar(stat = "identity") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          legend.position = "none") +
    ylab("Number of languages (truncated at 150)") +
    ggtitle("Number languages per measure")
```

Look at number of languages intersecting each variable 
```{r}
arealControl_vars = c("stock", "region", "lang.family", "lang.genus", 
                      "native.country", "native.country.area")

d.clean %>%
  gather(lang_measure, lang_value, 
         which(is.element(names(.), c(lang_vars, arealControl_vars)))) %>%
  group_by(lang_measure) %>%
  gather(pop_measure, pop_value,
         which(is.element(names(.), demo_vars))) %>%
  group_by(lang_measure, pop_measure) %>%
  mutate(both_not_na = !is.na(lang_value) & !is.na(pop_value)) %>%
  summarise(n_languages = length(which(both_not_na == TRUE))) %>%
  ggplot(aes(pop_measure, lang_measure, group = lang_measure)) +
      geom_tile(aes(fill = n_languages)) + 
      geom_text(aes(fill = n_languages, label = n_languages)) +
      scale_fill_gradient(low = "white", high = "red")  +
      xlab("population variables") + 
      ylab("language variables") + 
      ggtitle("Number languages between variables") +
      theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
     
## Relationships between variables
```{r,  fig.height = 10, fig.width = 10, eval = F}
Correlation matrix (stuff in the upper right quadrant matters)
is.na(d) <- sapply(d, is.infinite)
corrs = cor(d[,-c(1:3, 36)], use = "pairwise.complete.obs")
corrplot(corrs)
```

Scatter plots
```{r, fig.width=10, fig.height = 14, eval = F}

# these are the variables we care about for the paper
demo_vars_crit = c("n.neighbors_log",
              "mean.temp", "sum.precip", "sd.temp", "growing.season",
              "pop_log", "area_log" "n.neighbors_log",
              "sd.precip_log", "ratio.L2.L1_log", "distance.from.origin")

lang_vars_crit = c("p.complexity.bias","scaled.LDT.TTR", "mean.dependency.length",
              "mean.length", "mean.aoa",
              "n.consonants_log", "n.phonemes_log")

d.scatter = d.clean %>% 
  gather(lang_measure, lang_value, 
         which(is.element(names(.), lang_vars_crit))) %>%
  group_by(lang_measure) %>%
  gather(pop_measure, pop_value,
         which(is.element(names(.), demo_vars_vrit))) %>%
  select(pop_measure, lang_measure, pop_value, lang_value)

is.na(d.scatter) <- sapply(d.scatter, is.infinite)

# get correlations
d.scatter.corrs = d.scatter %>%
  group_by(lang_measure, pop_measure) %>%
  do(tidy(cor.test(~ .$pop_value + .$lang_value))) %>%
  mutate(sig = ifelse(p.value < .05, "*", "")) %>%
  mutate(sig.col = ifelse(p.value < .05 & estimate > 0, "pos",
                          ifelse(p.value < .05 & estimate < 0, "neg",
                                 "none"))) %>%
  mutate(pop_value = .1, lang_value = .1) %>% # this is a hack
  ungroup

ggplot(d.scatter, aes(x = pop_value, y = lang_value)) +
        geom_rect(data = d.scatter.corrs, aes(fill = sig.col), 
                xmin = -Inf, xmax = Inf,
                ymin = -Inf, ymax = Inf, alpha = 0.2) +
      geom_point(size = .3) +
      geom_smooth(method = "lm", color = "green") +
      facet_grid(lang_measure~pop_measure, scale = "free") +
    scale_fill_manual(name = "grp",values = c( "mediumblue", "grey99","red1")) +
    theme_bw() +
    xlab("Demographic variables") +
    ylab("Language variables") +
    theme(legend.position = "none") 

```


Fits controling for family -- controlling for family in a mixed effect model
```{r, fig.width=10, fig.height = 14, eval = F}

d.scatter.fam = d.clean %>% 
  filter(pop_log>2) %>%
  select(n.neighbors_log, mean.temp, sum.precip, sd.temp, growing.season, pop_log,
         area_log, n.neighbors_log, sd.precip_log, ratio.L2.L1_log, distance.from.origin,
         p.complexity.bias, scaled.LDT.TTR, mean.dependency.length, mean.length, mean.aoa,
         n.consonants_log, n.phonemes_log, lang.family, lang.genus) %>%
  gather(lang_measure, lang_value, 
         which(is.element(names(.), lang_vars_crit))) %>%
  group_by(lang_measure) %>%
  gather(pop_measure, pop_value,
         which(is.element(names(.), demo_vars_crit)))

is.na(d.scatter.fam) <- sapply(d.scatter.fam, is.infinite) 
d.scatter.fam <- filter(d.scatter.fam, !is.na(pop_value), !is.na(lang_value))

# get model fits
d.model.fits = d.scatter.fam %>%
  group_by(lang_measure, pop_measure) %>%
  do(tidy(lmer(lang_value ~ pop_value + (pop_value|lang.family) + (pop_vaue), data=.))) %>%
  filter(term == "pop_value") %>%
  mutate(sig = ifelse(abs(statistic) > 1.96, "*", "")) %>%
  mutate(sig.col = ifelse(statistic > 1.96, "pos",
                          ifelse(statistic < -1.96, "neg",
                                 "none"))) %>%
  mutate(pop_value = .1, lang_value = .1) %>% # this is a hack
  ungroup

ggplot(d.scatter.fam, aes(x = pop_value, y = lang_value)) +
        geom_rect(data = d.model.fits, aes(fill = sig.col), 
                xmin = -Inf, xmax = Inf,
                ymin = -Inf, ymax = Inf, alpha = 0.2) +
      geom_point(size = .3, aes(col = lang.family)) +
      geom_smooth(method = "lm", color = "green") +
      facet_grid(lang_measure~pop_measure, scales = "free") +
     scale_fill_manual(name = "grp", 
                       values = c( "mediumblue", "grey99","red1")) +
    theme_bw() +
    xlab("Demographic variables") +
    ylab("Language variables") +
    theme(legend.position = "none")
```


Fits controlling for language family (mean.fam.lang ~ mean.fam.pop)
Aggregating across families
```{r, fig.width=10, fig.height = 14, eval = F}
d.scatter.fam = d %>% 
  select(log.n.consonants, log.n.vow, son, log.asjp.mean.length, 
         H.LDT, lun.mean.aoa, p.complexity.bias, m_obs, 
         log.RatioL2, log.pop2, growingSeason, mean.temp, 
         log.sum.precip, log.area, langFamily) %>%
  rename(log.length = log.asjp.mean.length, lexical.diversity = H.LDT, 
         aoa = lun.mean.aoa, complexity.bias = p.complexity.bias,
         dependency.length = m_obs, log.n.vowels = log.n.vow) %>%
  group_by(langFamily) %>% 
  summarise_each(funs(mean(., na.rm = TRUE))) %>%
  gather(lang_measure, lang_value, 
         which(is.element(names(.), lang_vars))) %>%
  group_by(lang_measure) %>%
  gather(pop_measure, pop_value,
         which(is.element(names(.), demo_vars)))

is.na(d.scatter.fam) <- sapply(d.scatter.fam, is.infinite)

# get correlations
d.scatter.corrs = d.scatter.fam %>%
  group_by(lang_measure, pop_measure) %>%
  do(tidy(cor.test(~ .$pop_value + .$lang_value))) %>%
  mutate(sig = ifelse(p.value < .05, "*", "")) %>%
  mutate(sig.col = ifelse(p.value < .05 & estimate > 0, "pos",
                          ifelse(p.value < .05 & estimate < 0, "neg",
                                 "none"))) %>%
  mutate(lab = paste("r = ", round(estimate,2),sig, sep = "")) %>%
  mutate(pop_value = .1, lang_value = .1) %>% # this is a hack
  full_join(text.pos) %>%
  ungroup

ggplot(d.scatter.fam, aes(x = pop_value, y = lang_value)) +
        geom_rect(data = d.scatter.corrs, aes(fill = sig.col), 
                xmin = -Inf, xmax = Inf,
                ymin = -Inf, ymax = Inf, alpha = 0.2) +
      geom_point(size = .3) +
      geom_smooth(method = "lm", color = "green") +
      facet_grid(lang_measure~pop_measure, scale = "free") +
      #geom_text(aes(min_pop + .4*dif_pop, min_lang + .05*dif_lang, label=lab),
      #        data = d.scatter.corrs, color = "red") +
    scale_fill_manual(name = "grp", 
                      values = c( "mediumblue", "grey99","red1")) +
    theme_bw() +
    xlab("Demographic variables") +
    ylab("Language variables") +
    theme(legend.position = "none") 
```


Fits controling for family -- controling for family in a linear model
(lm(lang ~ pop + lang.family))
```{r, fig.width=10, fig.height = 14, eval = F}

#summary(lmer(lang_value~pop_value+ (1|langFamily), d.scatter))$coefficients["pop_value", "t value"]

d.scatter.fam2 = d %>% 
  filter(log.pop2>2) %>%
  select(log.n.consonants, log.n.vow, son,  log.asjp.mean.length,
         H.LDT, lun.mean.aoa, p.complexity.bias, m_obs, 
         log.RatioL2, log.pop2, growingSeason, mean.temp, 
         log.sum.precip, log.area,langFamily) %>%
  rename(log.length = log.asjp.mean.length, lexical.diversity = H.LDT, 
         aoa = lun.mean.aoa, complexity.bias = p.complexity.bias,
         dependency.length = m_obs, log.n.vowels = log.n.vow) %>%
  gather(lang_measure, lang_value, 
         which(is.element(names(.), lang_vars))) %>%
  group_by(lang_measure) %>%
  gather(pop_measure, pop_value,
         which(is.element(names(.), demo_vars)))

is.na(d.scatter.fam2) <- sapply(d.scatter.fam2, is.infinite) 
d.scatter.fam2 <- filter(d.scatter.fam2, !is.na(pop_value), !is.na(lang_value))

# get correlations
d.scatter.corrs = d.scatter.fam2 %>%
  group_by(lang_measure, pop_measure) %>%
  do(tidy(lm(lang_value ~ pop_value * langFamily, data = .))) %>%
  filter(term == "pop_value") %>%

   mutate(sig = ifelse(p.value < .05, "*", "")) %>%
   mutate(sig.col = ifelse(p.value < .05 & estimate > 0, "pos",
                           ifelse(p.value < .05 & estimate < 0, "neg",
                                  "none"))) %>%
  mutate(pop_value = .1, lang_value = .1) %>% # this is a hack
  ungroup

ggplot(d.scatter.fam2, aes(x = pop_value, y = lang_value)) +
        geom_rect(data = d.scatter.corrs, aes(fill = sig.col), 
                xmin = -Inf, xmax = Inf,
                ymin = -Inf, ymax = Inf, alpha = 0.2) +
      geom_point(size = .3, aes(col = langFamily)) +
      geom_smooth(method = "lm", color = "green") +
      facet_grid(lang_measure~pop_measure, scales = "free") +
     scale_fill_manual(name = "grp", 
                       values = c( "mediumblue", "grey99","red1")) +
    theme_bw() +
    xlab("Demographic variables") +
    ylab("Language variables") +
    theme(legend.position = "none")
```

## PCA
```{r}
```

SHINY APP
```{r, eval = F}
lmeasure <- "lexical.diversity"
pmeasure <- "log.RatioL2"
subdata <- filter(d.scatter.fam2,
                  lang_measure == "lmeasure", pop_measure == "pmeasure")
```

```{r, eval = F}
shinyApp(
  
  ui = fluidPage(
    selectInput("lmeasure", "Language Measure:", 
                choices = as.character(unique(d.scatter.fam2$lang_measure)),
                selected = "lexical.diversity"),
    selectInput("pmeasure", "Population Measure:", 
                choices = as.character(unique(d.scatter.fam2$pop_measure)),
                selected = "log.RatioL2"),
    plotOutput("scatterPlot"), 
    verbatimTextOutput("lmer")
  ),
  
  server = function(input, output) {
    output$scatterPlot <- renderPlot({
      qplot(pop_value, lang_value, 
            data = filter(d.scatter.fam2, lang_measure == input$lmeasure & 
                     pop_measure == input$pmeasure)) + 
        geom_smooth(method="lm") + 
        theme_bw()
    })
    
    output$lmer <- textOutput({
      summary(lmer(lang_value ~ pop_value + (pop_value | langFamily), 
                   data = filter(d.scatter.fam2, 
                                 lang_measure == input$lmeasure & 
                                   pop_measure == input$pmeasure)))
    })
  },
  
  options = list(height = 500)
)
```



WALS features of complexity
```{r, eval = F}
# aggregate across countries to get quantitative measures by language
most.frequent.level = function (x){
    mf = names(which.max(table(x)))
    return(mf)
}

# aggregate L&D data by language
qual = ld %>%
  colwise(as.factor)(.)  %>%
  select(-1:-17, -104:-122) %>%
  #select(-1:-3, -5:-9, -13, -15:-17, -104:-122) %>%
  group_by(eln2) %>%
  summarise_each(funs(most.frequent.level))

qual <- colwise(as.factor)(qual)

# include 
labNames= read.csv("../data/lupyan_WALS_feature_names.csv")
toInclude = labNames[labNames$include == 1, ]
qualVarNames = intersect(names(qual), toInclude$Name)

# Some vars are missing from L&D data set ["HASIND" "GILDIF" "AUWPRH" "DRYSBV" "DRYOBV" "DRYXOV" "DRYREL" "POLANT" "POLAPP" "SONPER" "SONNON" "NICMTP" "NICNMP" "DAHTEA"]

# remap factor levels to human readable values (not all present)
labMappings = read.csv("../data/lupyan_WALS_lab_mappings.csv")
for (i in 1:length(qualVarNames)){
  thisVarLabs = labMappings[labMappings$featureName == qualVarNames[i],]
  old = thisVarLabs$oldLab
  new = thisVarLabs$newLab
  col_i = grep(qualVarNames[i], colnames(qual))
  qual[,col_i] = mapvalues(qual[,col_i], 
                           from = as.character(old),
                           to = as.character(new))
}

featureCats = levels(droplevels(toInclude$Feature.Class))
```

## Parallels between language ontogeny and evolution
get measures of developmental change in vocabulary

## Learnability pressures predicts AOA across languages
Predict AOA by social variables, by social variable
```{r, fig.width=10, fig.height = 10, eval = F}
aoa_central = aoa_data %>% 
  mutate(language = tolower(language)) %>%
  group_by(language, measure, lexical_category) %>%
  summarise(mean_aoa = mean(aoa))
  
d3 = inner_join(d, aoa_central, by = "language") %>%
  select(language, log.RatioL2, log.pop2,
         log.numNeighbors, log.area, 
         mean.temp, mean_aoa, measure, lexical_category) %>%
  mutate(language = as.factor(language)) %>%
  gather(pop_measure, pop_value, 2:6) 

ggplot(d3, aes(y = mean_aoa, 
              x = pop_value,
              label = language,
              group = measure,
              color = measure)) +
  geom_text(size = 3, aes(group = measure)) +
  facet_grid(lexical_category ~ pop_measure, scales = "free") +
  geom_smooth(method = "lm") +
  theme_bw() 
````